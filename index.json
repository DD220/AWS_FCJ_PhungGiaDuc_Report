[{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Phung Gia Duc\nPhone Number: 0962216625\nEmail: ducpgse183187@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Assurance\nClass: FCAJ\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud\nInternship Duration: From 08/09/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"FCJ 3-month worklog: Week 1: Week 1\nWeek 2: Week 2\nWeek 3: Week 3\nWeek 4: Week 4\nWeek 5: Week 5\nWeek 6: Week 6\nWeek 7: Week 7\nWeek 8: Week 8\nWeek 9: Week 9\nWeek 10: Week 10\nWeek 11: Week 11\nWeek 12: Week 12\n"},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"AWS Cloud Mastery Series #1 Event Objectives Provide an overview of AWS’s AI/ML ecosystem Introduce how to build and apply Generative AI using Amazon Bedrock Key Highlights 1. Overview of AI/ML \u0026amp; AWS Ecosystem Discussed the development trends of AI/ML/GenAI in Vietnam Explained why training, optimizing, and deploying models on the cloud is essential Presented the purpose and direction of the Cloud Mastery Series workshops 2. AWS AI/ML Services Overview Main topics covered:\nAmazon SageMaker: a full ML environment for building, training, fine-tuning, and deploying models Data standardization \u0026amp; labeling: improving dataset quality MLOps: automating the AI lifecycle, managing model and data versions Live Demo: hands-on with SageMaker Studio – creating notebooks, training, and deploying models 3. Generative AI with Amazon Bedrock Key hands-on topics:\nFoundation Models (Claude, Llama, Titan) Comparison and selection of suitable models for real-world use cases Prompt Engineering Basic prompting Few-shot prompting Chain-of-Thought reasoning RAG Architecture Integrating enterprise data into GenAI systems Building a Knowledge Base to improve accuracy Bedrock Agents \u0026amp; Guardrails Multi-step task orchestration Defining safety rules and content control Live Demo Building a complete GenAI chatbot from scratch using Amazon Bedrock Key Takeaways Design Mindset AI-first thinking: start from the business problem, then choose the model Responsible AI: prioritize safety, compliance, and transparency Build a shared language between engineering, product, and business teams Technical Architecture Modern ML workflow: ingest → preprocess → train → deploy → monitor RAG \u0026amp; Agents: how to implement internal chatbots or advisory systems Backend integration patterns with Foundation Models MLOps reduces errors and shortens the development lifecycle Modernization Strategy Standardize ML processes using SageMaker Apply Guardrails for safe GenAI deployment Optimize cost by selecting efficient models and automating pipelines Real-world applications: chatbots, content classification, customer support, Q\u0026amp;A systems Applying to Work Designing pipelines from data standardization to model monitoring Applying RAG to internal products Building chatbots serving enterprise operations Selecting models based on accuracy – cost – speed Integrating Bedrock into applications to enhance performance Event Experience Insights from AWS Experts Learned directly from AWS teams about how enterprises adopt GenAI Gained clarity on building pipelines, selecting models, and optimizing AI systems Technical Experience Became more familiar with the SageMaker Studio interface Gained solid understanding of RAG, Agents, and the strengths of each foundation model Practiced various Prompt Engineering techniques Modern Tools Built a GenAI chatbot quickly with Bedrock Learned how to use Guardrails for content moderation Event Significance Created a bridge between the AI/ML community and AWS experts Helped define clear directions for building GenAI products Fostered a unified language across ML – Data – DevOps – Business teams Lessons Learned GenAI is a complete architecture, not just a model RAG enables models to understand enterprise data more deeply MLOps is essential for operating AI at scale Choosing a Foundation Model must be based on real needs, not trends "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Solution for Handling DCA Complaints in the Motor Finance Industry Using AWS Services Authors: Kaushal Goyal, Dogus Gucsav, and Pardeep Kumar\nPublished: June 27, 2025\nCategories: Amazon AppFlow, Amazon Bedrock, Amazon CloudWatch, Amazon Connect, Amazon DynamoDB, Amazon Pinpoint, Amazon QuickSight, Amazon VPC, AWS CloudTrail, AWS Database Migration Service, AWS Glue, AWS IAM, AWS KMS, AWS Lambda, AWS Step Functions, Financial Services, Generative AI, Industries\nOverview The UK Motor Finance industry is facing significant challenges following the FCA’s review of Discretionary Commission Arrangements (DCA). Financial institutions must analyze historical lending data, calculate accurate redress amounts, and manage large volumes of customer interactions — all within tight regulatory deadlines and fragmented data environments.\nThis article explains how AWS services can help financial organizations build a scalable solution to process DCA complaints while laying the foundation for future regulatory requirements.\nKey Challenges Processing large volumes of historical customer data stored across multiple legacy systems Extracting relevant information from legacy documents and unstructured data Calculating compensation using complex business rules Handling high volumes of customer requests across multiple channels Ensuring GDPR compliance and meeting regulatory standards Achieving all of the above within strict timelines Solution Architecture The architecture consists of four major components, forming a complete data/document processing pipeline, a redress calculation engine, and a customer communication hub.\n1. Intelligent Document Processing Uses Generative AI on Amazon Bedrock to analyze financial agreements and extract:\nOriginal APR Effective applied interest rate Broker commission amounts Sales documentation This automated process identifies affected customers and the potential extent of overcharging.\nDocuments are uploaded to Amazon S3 via AWS DataSync or AWS Transfer Family.\n2. Data Ingestion \u0026amp; Preparation AWS DMS securely migrates legacy motor finance and customer data to AWS Data is stored in Amazon S3 as a scalable, secure data lake AWS Glue crawlers automatically catalog customer data AWS Glue jobs cleanse and enrich data Amazon AppFlow ingests customer data from SaaS platforms (Salesforce, SAP, ServiceNow, etc.) This prepares customer datasets for redress analysis.\n3. Redress Calculation Engine A scalable, serverless architecture is used to compute redress:\nAWS Lambda performs large-scale redress calculations Amazon DynamoDB stores business rules for consistent processing AWS Step Functions orchestrate workflows Amazon QuickSight provides real-time dashboards for monitoring progress and financial impact The design auto-scales during peak complaint periods and maintains detailed audit logs for FCA compliance.\n4. Customer Communication Hub Amazon Connect serves as the AI-powered contact center Amazon Pinpoint delivers multi-channel personalized messaging AWS Amplify hosts secure self-service portals Amazon AppFlow integrates with CRM systems (Salesforce, SAP, etc.) Amazon Q in QuickSight allows natural-language queries for rapid investigation This ensures consistent and scalable customer communication.\nSecurity \u0026amp; Compliance The entire solution is designed with strong security controls:\nNetwork isolation via Amazon VPC AWS KMS for encryption of sensitive data AWS Secrets Manager for secret management AWS IAM for granular access control Amazon CloudWatch \u0026amp; AWS CloudTrail for monitoring and auditability The architecture complies with FCA and GDPR, providing required audit trails.\nBenefits \u0026amp; Conclusion With AWS, financial institutions can:\nEfficiently process large volumes of DCA complaints Maintain regulatory compliance Improve operational efficiency Build a long-term data foundation for future regulatory needs The serverless architecture optimizes cost, scaling automatically based on demand. Fully managed services significantly reduce operational overhead.\nOrganizations should assess their individual requirements to determine the best implementation approach.\nTo learn more about how AWS can support your regulatory challenges, contact your AWS account team or an AWS Financial Services Competency Partner.\nAuthors Kaushal Goyal Solutions Architect at AWS supporting enterprise customers in UK\u0026amp;I Financial Services. Experienced in modernizing legacy systems and building cloud-native architectures. Passionate about Generative AI and container technologies.\nDogus Gucsav Senior Solutions Architect at AWS specializing in cloud-native solutions for Financial Services. Helps banks achieve transformation goals using cloud-native architectures, AI, and composable banking principles.\nPardeep Kumar Data Architect with AWS Professional Services. Specializes in analytics and Generative AI solutions that deliver measurable business value. Designs secure, scalable, and future-ready data platforms.\n"},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Research at the Speed of Discovery: How AWS Cloud Bursting Eliminates Computational Bottlenecks Imagine you’ve made a promising breakthrough in genomic research that could lead to a major advancement in cancer treatment. The preliminary results look strong — but running the full analysis will consume three weeks of your university’s entire GPU cluster. Meanwhile, other researchers are waiting in line with equally urgent projects.\nShould you wait and lose momentum on a critical discovery?\nOr should you scale down your analysis, risking the loss of important insights?\nThis dilemma happens every day in research institutions — forcing scientists to choose between ambition and available compute capacity.\nThe Dilemma of Research Computing Research compute needs are inherently bursty:\nperiods of extremely high demand during breakthroughs, funding deadlines, or conference submissions followed by quieter periods of lower computational need This pattern fundamentally mismatches traditional High Performance Computing (HPC) systems.\nMost university HPC clusters are undersized by design due to budget constraints. Institutions can only afford systems that meet the average workload, not peak demand. The problem is especially severe for specialized resources like GPU nodes or high-memory systems, where wait times often stretch to days or weeks.\nWhy Traditional Approaches Fall Short Fixed-capacity HPC cannot handle overlapping spikes from hundreds of researchers Tight budgets force slow hardware refresh cycles (3–5 years) On-prem hardware becomes outdated while cloud infrastructure evolves continuously Research momentum slows when breakthrough ideas must wait for compute access The performance gap widens every year Even systems running at high utilization simply cannot meet unpredictable surges in workload.\nHow HPC Bursting Works AWS partnered with Pariveda Solutions (AWS Premier Consulting Partner) to build a flexible HPC bursting solution that augments on-prem clusters with cloud capacity on demand — without requiring massive capital investment.\nKey Components: AWS Plugin for Slurm: integrates seamlessly with existing Slurm clusters Automated provisioning and cleanup of cloud resources Budget controls and guardrails for responsible cloud use Researchers keep their same tools, scripts, and workflow — no retraining needed Workflow: Submit jobs to the on-prem HPC cluster as usual Slurm determines whether the job should burst to AWS Jobs are securely transferred using VPN Site-to-Site or AWS Direct Connect AWS provisions the required EC2 instances automatically Jobs run on the cloud Instances shut down when jobs finish — minimizing cost Your workflow stays the same — only the capacity increases.\nGoing Beyond Your Physical Capacity: Access to Advanced Resources HPC bursting gives instant access to hardware that is often impossible for universities to purchase:\nLatest-generation NVIDIA GPUs Modern Intel and AMD CPU architectures ARM-based AWS Graviton processors optimized for cost and performance AWS Trainium and Inferentia chips for AI/ML acceleration FPGAs for specialized computations Quantum computing systems via Amazon Braket Instead of waiting weeks for a GPU node, you get instant availability at massive scale.\nSeamless Data Access A key requirement for HPC bursting: data must remain accessible regardless of where the job executes.\nCloud bursting supports this through several storage options:\nAmazon EFS — mounted like an on-prem network drive Amazon S3 — unlimited object storage for large datasets Amazon FSx for Lustre — high-performance parallel file system for HPC Direct mount of on-prem storage into the cloud environment Researchers see the same data whether jobs run on-prem or on AWS.\nReal-World Impact Cloud bursting has generated meaningful results across research fields:\nClimate modeling teams run ensemble forecasts in hours instead of weeks AI researchers access the newest NVIDIA GPUs and maintain research velocity Computational chemistry labs scale from 100 → 10,000 cores for drug discovery Materials science teams combine classical HPC with quantum computing Benefits for Researchers and IT Teams For Researchers: Near-zero learning curve Existing scripts, workflows, and authentication remain unchanged No need to wait days/weeks for GPU availability Continue ambitious projects without compromise For IT Teams: Intelligent load balancing across on-prem and cloud No more purchasing expensive hardware that sits idle most of the time Unified monitoring for hybrid environments Cost model shifts from large capital expenditures to usage-based operational spend The Economics of HPC Bursting Typically 60–80% cheaper than buying equivalent dedicated hardware Most institutions achieve ROI within 6–12 months Eliminates multi-million-dollar hardware refresh cycles Enables research budgets to scale with actual activity Getting Started Cloud bursting is usually deployed in three phases:\n1. Assessment Identify pilot research groups Review network requirements Define security and compliance needs 2. Infrastructure Setup Configure secure connectivity Integrate cloud bursting with the existing HPC environment Set up monitoring and observability 3. Pilot Deployment Run test jobs on burst capacity Tune workflows and scheduling policies Optimize cost and performance Jobs typically launch in 2–3 minutes, and data sync occurs automatically in the background.\nMost workflows require modest network bandwidth, but data-heavy applications benefit from AWS Direct Connect.\nThe Future of Research Computing Hybrid HPC — on-prem plus cloud bursting — represents the future of scientific computation as workloads become increasingly data-intensive and complex.\nEmerging compute models like quantum computing can be accessed through cloud bursting, ensuring your institution stays at the cutting edge.\nCloud bursting eliminates computational bottlenecks that slow scientific progress, ensuring breakthroughs are not held back by infrastructure limitations.\nResearchers no longer plan projects based on available hardware.\nThey plan based on scientific opportunity.\n"},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"AWS Transform for .NET Now Supports Q\u0026amp;A for Assessment and Transformation Reports Authors: David Pallmann, Vijay Mandadi, Rakshith Ravi Kumar\nPublished: July 1, 2025\nCategories: .NET, AWS Transform\nIntroduction AWS Transform for .NET accelerates large-scale modernization of .NET applications, including migrating from .NET Framework to cross-platform .NET and upgrading between .NET versions.\nToday, we are introducing a new chat-based Q\u0026amp;A feature that allows you to obtain insights about your transformation jobs directly from the web console using natural language.\nThis new capability lets you ask questions about assessment reports and transformation reports without manually browsing through long documents—especially useful when working with multiple repositories.\n(Note: The chat feature is currently available only in the web console, not in IDE integrations.)\nAssessment Queries When you configure a .NET transformation job, AWS Transform first evaluates the selected repositories.\nYou can download the assessment report in HTML or JSON format from the Collaboration tab.\nThe assessment report provides details such as:\nRepository owner Branch evaluated Number of solutions Number of projects Total lines of code Detected .NET versions Project types Public/private NuGet dependencies Migration complexity Once the assessment is ready, the Worklog tab will display the message:\n“Assessment report is now available in chat for queries.”\nTo open the chat panel, click the purple hexagon icon at the bottom-right corner of the console.\nExample Assessment Queries: The chat interface will interpret the assessment report and respond accordingly.\nTransformation Queries After the transformation of a repository is completed—or when the entire job finishes—AWS Transform generates a transformation report.\nThe Dashboard tab displays the status of each repository:\nIn-progress Success Failed You may download the transformation report or query it directly via chat.\nExample Transformation Queries: Example:\nAsking “What changes were made to the hello-bedrock repository?” returns details on the migration from .NET 6 to .NET 8, Bedrock SDK updates, project structure improvements, and migration-analysis changes. The response also confirms that the transformation succeeded.\nAsking “What packages were upgraded in the mathcore-main project?” lists package upgrades such as NUnit, FluentAssertions, and Microsoft.NET.Test.Sdk.\nProviding Feedback If the chat feature is unable to interpret your query, you may receive a generic fallback response.\nTo improve accuracy:\nClarify whether your question pertains to the assessment report or the transformation report Review the downloaded reports to better understand their structure You can help AWS refine this capability by using the thumbs up / thumbs down icons under each chat response.\nSelecting one will open a prompt for additional feedback—greatly appreciated by AWS.\nConclusion This article demonstrates how to use AWS Transform for .NET’s natural language chat feature to explore:\nRepository assessment reports Transformation reports The chat interface streamlines understanding of complex modernization workflows, enabling faster and more efficient .NET application upgrades.\nAbout the Authors David Pallmann Senior Product Manager on the AWS Transform team, specializing in the .NET developer experience.\nFormer engineer, consultant, product manager, and engineering manager.\nPreviously worked on WCF and created Neuron ESB, the first .NET-based enterprise service bus.\nFollow on X: @davidpallmann\nVijay Mandadi Experienced technical leader in AWS Migrations and Modernizations with over 16 years of expertise across distributed systems, cloud computing, virtualization, workload transformation, and healthcare.\nAt AWS, he focuses on leveraging Generative AI and Agentic AI to accelerate customer modernization efforts and enable cloud-native architectures.\nRakshith Ravi Kumar Senior Software Engineer at AWS with more than 11 years of experience in software development.\nExpertise includes mobile development, CRM systems, NetBackup solutions, and cloud migration technologies.\nCurrently working on agentic solutions for modernization and migration workloads.\n"},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"AWS Cloud Mastery Series #2 Event Objectives Understand the DevOps cultural foundation and how to apply it in enterprises Master the process of building CI/CD pipelines using AWS DevOps services Practice Infrastructure as Code (IaC) with CloudFormation \u0026amp; CDK Explore containerization, system monitoring, and performance observability Key Highlights 1. DevOps Mindset \u0026amp; Cultural Foundation The session began with a brief recap of the previous AI/ML content and then moved into the main topics:\nStrengthening communication between technical teams Improving development speed using automation and fast feedback loops Introducing key DevOps metrics: DORA metrics, MTTR, deployment frequency Highlighting the importance of DevOps in building scalable and resilient systems 2. AWS DevOps Services – Building a CI/CD Pipeline A complete DevOps-native workflow on AWS was presented:\nSource Control\nAWS CodeCommit, GitFlow, and Trunk-based development Build \u0026amp; Test\nCreating build specifications using CodeBuild Automatically running unit tests / integration tests within the pipeline Deployment\nCodeDeploy with Blue/Green strategies Rolling \u0026amp; Canary deployments to reduce production deployment risks Orchestration\nCodePipeline to bring all CI/CD stages together into one automated flow Live Demo\nDemonstrating a full pipeline from commit → build → deploy 3. Infrastructure as Code (IaC) Deep-dive topics included:\nAWS CloudFormation Declaring infrastructure using templates Deploying, managing stacks, and handling drift AWS CDK Defining AWS resources using code Reusing architectural patterns with multi-language support (Python, TS, Java…) Comparison: When to choose CDK vs CloudFormation Live Demo\nDeploying infrastructure with both CloudFormation and CDK Key Takeaways DevOps Mindset Prioritize automation to reduce manual errors Deploy continuously with smaller, safer releases Make decisions based on measurable metrics (DORA, MTTR) Technical Architecture Build flexible and scalable CI/CD pipelines IaC ensures consistency and minimizes configuration drift Choose appropriate container services: ECS, EKS, or App Runner Observability is a core component of system operations Modernization Strategy Migrate applications to microservices + containers Standardize infrastructure across environments using IaC Use blue/green \u0026amp; canary strategies for safer deployments Strengthen monitoring with CloudWatch and X-Ray Applying to Work Build CI/CD pipelines with CodePipeline + CodeBuild + CodeDeploy Replace manual configurations using IaC Containerize workloads and evaluate ECS/EKS/App Runner Apply DevOps best practices: A/B testing, automatic rollback Event Experience Learning from AWS Experts Understand DevOps journeys from real enterprises Gain clarity on how CI/CD accelerates development while reducing downtime Hands-on Technical Experience Detailed walkthrough of each CI/CD pipeline stage Practiced IaC using CloudFormation \u0026amp; CDK Full demo of containerization and observability Modern Tools Docker \u0026amp; ECR for container workflows Monitoring stack from CloudWatch to X-Ray Practical Insights Real incident scenarios and postmortem processes Safe deployment techniques using feature flags How large organizations operate DevOps at scale Lessons Learned DevOps is culture + automation, not just tooling IaC is the key to speed and accuracy in infrastructure management Containers + CI/CD dramatically accelerate release cycles Observability is critical for maintaining large-scale systems "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"AWS Well-Architected Framework – Security Pillar Workshop Event Objectives Deep-dive into the Security Pillar of the AWS Well-Architected Framework\nUnderstand modern security best practices: Least Privilege, Zero Trust, Defense in Depth\nLearn how to design secure cloud environments with Identity, Detection, Infrastructure Protection, Data Protection, and Incident Response pillars\nKey Highlights Security Foundation Explained the role of the Security Pillar within the Well-Architected Framework Core principles: Least Privilege — minimize access permissions\nZero Trust — never trust, always verify\nDefense in Depth — layered security approach\nReview of the Shared Responsibility Model and how it applies to AWS services Overview of common cloud threats in Vietnam, including misconfigurations, credential compromise, and public-data exposure Identity \u0026amp; Access Management (IAM) Modern IAM Architecture:\nIAM concepts: Users, Roles, Policies, and avoiding long-term credentials\nIdentity modernization with IAM Identity Center (SSO, permission sets)\nGovernance for multi-account environments via Service Control Policies (SCP) and permission boundaries\nStrengthening access control with MFA, credential rotation, and Access Analyzer\nMini Demo: IAM Policy validation and access simulation\nDetection Detection \u0026amp; Continuous Monitoring:\nUnified logging and monitoring using: CloudTrail (organization-level)\nGuardDuty threat detection\nSecurity Hub compliance findings\nLogging across all layers: VPC Flow Logs, ALB logs, S3 Access Logs\nAutomated alerting with EventBridge\nConcept of Detection-as-Code for repeatable and auditable monitoring rules\nInfrastructure Protection Network \u0026amp; Workload Security:\nDesigning secure networks with VPC segmentation and correct use of public vs private subnets\nComparison of Security Groups vs NACLs with recommended patterns\nLayers of perimeter defense: AWS WAF, Shield, Network Firewall\nWorkload-level protection for EC2, ECS, and EKS environments\nData Protection Encryption, Keys \u0026amp; Secrets:\nDeep dive into AWS KMS: key policies, grants, rotation strategies\nEncryption best practices for AWS services (S3, EBS, RDS, DynamoDB)\nManaging sensitive configuration with Secrets Manager and Parameter Store\nData classification and guardrails aligned with compliance and governance needs\nIncident Response AWS Incident Response lifecycle\nWalkthrough of key playbooks:\nCompromised IAM key containment\nS3 public exposure remediation\nEC2 malware detection response steps\nTechniques for snapshot creation, resource isolation, and evidence collection\nAutomated IR workflows using Lambda and Step Functions\nWrap-Up \u0026amp; Q\u0026amp;A Summary of all 5 Security Pillars\nDiscussion on common pitfalls in Vietnamese cloud deployments\nRecommended learning path:\nAWS Security Specialty\nAWS Solutions Architect Professional\nKey Takeaways Security Design Mindset Security must be built from the foundation, not added later\nConsistent enforcement of least privilege and strong identity controls\nVisibility is essential—log everything, monitor continuously\nTechnical Implementation Adopt centralized identity with IAM Identity Center\nUse organization-wide logging and threat detection tools\nApply network segmentation and layered protection for workloads\nImplement encryption, key management, and secret rotation systematically\nOperational Excellence Build repeatable incident response playbooks\nAutomate detection and remediation where possible\nEstablish governance practices across multi-account AWS environments\nApplying to Work Review and refactor existing IAM structure following modern IAM patterns\nIntroduce organization-level CloudTrail and centralized logging\nStrengthen infrastructure controls with VPC segmentation + WAF/Shield\nImplement encryption standards and secret rotation policies\nBuild IR playbooks and integrate automation into response workflows\nEvent Experience Attending this workshop provided a structured, comprehensive, and practical understanding of AWS cloud security. Key reflections include:\nLearning from highly skilled speakers In-depth guidance from AWS security specialists\nReal-world examples of cloud security incidents in Vietnam\nHands-on demonstrations Practical IAM policy simulation\nStep-by-step examples of threat detection and IR automation\nStrategic mindset shift Understanding that security is not only technical—it requires governance, culture, and continuous improvement\nNetworking opportunities Engaging discussions with AWS experts and peers on real security challenges faced by enterprises\nOverall, this workshop strengthened both my technical skills and strategic understanding of cloud security, helping me design more secure, resilient, and compliant architectures.\n"},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"DevSecOps Pipeline Overview (GitLab → AWS) Pipeline goals: Automate the entire build process – security scanning – error notification. Detect bugs and security vulnerabilities early as soon as dev commits code. Create a closed DevSecOps loop: Commit → Scan → Notify → Fix → Commit again.\nFlow summary: Dev commits code to GitLab (main branch). AWS CodePipeline receives events and triggers the pipeline. CodeBuild runs Sonar Scanner to analyze the source code. SonarQube on EC2 receives scan results from Scanner. SonarQube sends Webhook → API Gateway → Lambda. Lambda processes data → sends notification via SNS → dev email. Dev receives bug report → Fix → Commit → go back to loop. The system ensures: Automate security testing. Improve source code quality. Reduce security risks and logic errors. "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Start getting familiar with the workflow in the First Cloud Journey program. Tasks to be carried out this week: Day Task Date Reference Material 2 - Get to know the FCJ members - Read the onboarding documents and review the internship rules and guidelines 08/09/2025 https://policies.fcjuni.com/ 3 - Practice: + Create AWS account + Setup with Virtual MFA Device + Create admin group and admin user + Account authentication support 09/09/2025 https://000001.awsstudygroup.com/vi/ 4 - Practice: + Create Budget by Template + Create Cost Budget Tutorial + Creating a Usage Budget in AWS 10/09/2025 https://000007.awsstudygroup.com/vi/ 5 Practice + Creating a Reservation Instance + Creating a Saving Plans Budget + Clean up Budget 11/09/2025 https://000007.awsstudygroup.com/vi/ 6 - Practice: + AWS support packages + Types of support requests + Change support package + Manage support requests 12/09/2025 https://000009.awsstudygroup.com/vi/ Week 1 Achievements: Gained an understanding of the basic concepts of cloud infrastructure and core AWS service groups Successfully created an AWS Free Tier account and configured essential security settings. Learned how to navigate and operate within the AWS Management Console. "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Gain a solid understanding of AWS IAM, Amazon VPC, and hands-on Amazon EC2. Understand core cloud concepts, AWS, and account management. Familiarize with cost optimization aspects and AWS Support. Complete the labs from Module 01. Tasks to be carried out this week: Day Task Date Reference Material 2 - AWS IAM - Hands-on: + Create AdminGroup, AdminUser and successfully sign in + Create an IAM Role, OperatorUser and perform Switch Rolee + Clean up unnecessary IAM users 15/09/2025 https://000002.awsstudygroup.com/en/ 3 - Amazon VPC \u0026amp; Site-to-Site VPN - Hands-on: + Create EC2 Public \u0026amp; Private instances + Create Elastic IP, NAT Gateway, update Route Tables and Subnet Associations + Create EC2 Instance Connect Endpoint + Create VPC, configure Subnets + Internet Gateway 16/09/2025 https://000002.awsstudygroup.com/en/ 4 - Amazon EC2 - Hands-on: + Create and successfully connect Windows \u0026amp; Linux instances + Test EC2 Key Pair loss scenarios, restrict resources via IAM 17/09/2025 https://000004.awsstudygroup.com/en/ 5 - Database \u0026amp; Storage Foundation - Hands-on: + AWS Storage Classes, Block Storage + AWS Managed Database Services 18/09/2025 Week 2 Achievements: Understand how the cloud works and how AWS differs from traditional approaches. Understand how to manage users, groups, roles, and permissions. Able to distinguish AWS support plans and know how to open a support ticket. Completed all of Module 01. Know what components make up AWS infrastructure and how to quickly find services on the Console. Familiar with cost management—able to create cost budgets and usage budgets, and tried creating a Savings Plans budget. "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Deep hands-on practice with IAM Roles in AWS IAM. Get familiar with and leverage the AWS Cloud9 development environment. Practice core operations with the Amazon S3 storage service. Amazon EKS Amazon ECS and AWS Fargate Tasks to be carried out this week: Day Tasks Date Source Materials 2 - IAM Role (AWS IAM) - Hands-on: + Successfully create a new S3 Bucket + Create an IAM User and Access Key + Initialize and attach an IAM Role to EC2 successfully 22/09/2025 https://000048.awsstudygroup.com/en/ 3 - AWS Cloud9 - Hands-on: + Create a Cloud9 environment + Use key features such as the Terminal, file editing… + Use the AWS CLI to list EC2 instances 23/09/2025 https://000049.awsstudygroup.com/en/ 4 - Amazon S3 - Hands-on: + Enable static website hosting + Configure public objects + Integrate CloudFront to accelerate the website + Enable bucket versioning + Delete unnecessary resources 24/09/2025 https://000057.awsstudygroup.com/en/ 5 - Amazon EKS - Hands-on: + EKS overview + EKS endpoint access + Networking for EKS pods + Exposing applications + EKS storage + Autoscaling in EKS 25/09/2025 6 - Amazon ECS and Fargate - Hands-on: + Introduction to ECS and Fargate + ECS overview + Architecture of ECS service + Overview of AWS Fargate + Integrating ECS with other AWS services 26/09/2025 Week 3 Achievements: AWS IAM (IAM Role) Clear understanding of the roles and relationships between User – Role – Policy; able to apply proper security best-practice permissions for AWS systems. AWS Cloud9 Proficient with the Cloud9 IDE; effective use of a cloud-based development environment without local setup. Amazon S3 Understand the Bucket – Object model and key S3 features for data management, permissions, and storage. Amazon EKS Understand Kubernetes architecture on AWS, operate main components and know networking, scaling and storage mechanisms. Amazon ECS \u0026amp; AWS Fargate Skilled in deploying containers with ECS/Fargate, know how to optimize and automate container operations on AWS. "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Continue advanced hands-on practice with Amazon RDS. Observability \u0026amp; Cost Management. Migrate \u0026amp; Modernize with AWS. Multi-account AWS environment management. Migration Factory. AWS Lambda and API Gateway. Messaging Services. Building a Data Platform on AWS. Tasks to be carried out this week: Day Tasks Date Source Materials 2 - Amazon RDS - Hands-on: + Prepare the VPC + Launch EC2 and RDS within the same Security Group + Create a DB in a Subnet Group + Build an EC2 instance to host the application + Complete creating the RDS database instance 29/09/2025 https://000005.awsstudygroup.com/en/ 3 - Observability \u0026amp; Cost Management - Hands-on: + Best practices for system observability + AWS observability tools + Cost considerations for ECS/EKS - Migrate \u0026amp; Modernize with AWS - Hands-on: + Migration journey process + Migration Evaluator + ADS \u0026amp; Migration Hub + 7R analysis + MRA \u0026amp; MPA + Application Migration Service 30/09/2025 https://specialforce.awsstudygroup.com/#/course/week-7:-migrate-\u0026-modernize-with-aws 4 - Multi-Account Governance - Hands-on: + Govern multi-account AWS environments + Deploy AWS Control Tower + Landing Zone best practices - Migration Factory - Hands-on: + Learn AWS MGN + Migration Factory model + Disaster Recovery on AWS + Reasons to use DR-based cloud + App2Container for application modernization 01/10/2025 https://specialforce.awsstudygroup.com/#/course/week-8:-managing-and-governing-multi-account-aws-environments./ 5 - Lambda \u0026amp; API Gateway - Hands-on: + Introduction to serverless + How Lambda works + Best practices + API Gateway + SAM – Serverless Application Model + Lambda/API GW quiz - Messaging Services - Hands-on: + Messaging \u0026amp; EDA overview + SQS + SNS + EventBridge \u0026amp; Schemas + Use cases 02/10/2025 https://specialforce.awsstudygroup.com/#/course/week-10:-lambda-and-api-gateway 6 - Building Data Platform - Hands-on: + Data Lake on AWS + Ingestion layer + Data transformation - Building Data Platform 2 - Hands-on: + Data governance + Data warehouse + Orchestration + Week 13 quiz 03/10/2025 https://specialforce.awsstudygroup.com/#/course/week-12:-building-data-platfrom-on-aws-(part-1) https://specialforce.awsstudygroup.com/#/course/week-13:-building-data-platfrom-on-aws-(part-2) Week 4 Achievements: Amazon RDS Proficient in setting up and operating RDS services, including connection configuration, security, and monitoring. Week 6 – Observability \u0026amp; Cost Management Able to monitor, analyze, and optimize performance–cost tradeoffs for AWS systems at enterprise scale. Migrate \u0026amp; Modernize with AWS Understand the full migration and modernization lifecycle and proficient with assessment and migration tools. Multi-Account Governance Know how to organize, monitor, and protect multi-account AWS environments using Control Tower and Landing Zone. Migration Factory Able to operate automated migration models, DR, and application modernization workflows using AWS tools. Lambda \u0026amp; API Gateway Build and deploy complete serverless applications on AWS. Messaging Services Deep understanding of asynchronous processing, event-driven architectures, and large-scale messaging patterns. Building Data Platform Build end-to-end data platforms with Data Lake, Data Warehouse, and orchestration on AWS. "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"AWS Security Scan Project – Project Plan [Team DevSecOps FCJ] – [FPT University / Internship Program] – [AWS Security Scan Project] Date: 2025-10-11\nTABLE OF CONTENTS 1.BACKGROUND AND MOTIVATION\n1.1 EXECUTIVE SUMMARY\n1.2 PROJECT SUCCESS CRITERIA\n1.3 ASSUMPTIONS\n2.SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM\n2.1 TECHNICAL ARCHITECTURE DIAGRAM\n2.2 TECHNICAL PLAN\n2.3 PROJECT PLAN\n2.4 SECURITY CONSIDERATIONS\n3.ACTIVITIES AND DELIVERABLES\n3.1 ACTIVITIES AND DELIVERABLES\n3.2 OUT OF SCOPE\n3.3 PATH TO PRODUCTION\n4.EXPECTED AWS COST BREAKDOWN BY SERVICES\n5.TEAM\n6.RESOURCES \u0026amp; COST ESTIMATES\n7.ACCEPTANCE\n1. BACKGROUND AND MOTIVATION 1.1 EXECUTIVE SUMMARY The AWS Security Scan Project aims to automate the security inspection process across the software development lifecycle by integrating AWS services such as CodePipeline, CodeBuild, CodeGuru Reviewer, and AWS Security Hub. This initiative enhances the security posture of continuous integration and deployment pipelines by embedding automated vulnerability scanning, AI-powered code analysis, and centralized incident monitoring. Use cases include:\nContinuous integration with built-in security validation.\nAutomated alerts and compliance reporting.\nReal-time visibility into vulnerabilities and code quality.\nPartner services focus on designing, implementing, and optimizing a DevSecOps pipeline that ensures secure, compliant, and efficient software delivery.\n1.2 PROJECT SUCCESS CRITERIA ≥95% of code commits pass automated security scans before deployment.\nReal-time alerts are sent within 2 minutes of anomaly detection.\nSecurity Hub compliance score ≥90%.\nSuccessful integration between CodePipeline, CodeBuild, and Security Hub with zero manual intervention.\n1.3 ASSUMPTIONS All AWS accounts are pre-configured with IAM roles and permissions for CodePipeline and CodeBuild.\nGitLab repository access and webhooks are enabled.\nSecurity tools (e.g., Trivy, Bandit, SonarQube) are available in CodeBuild environment.\nThe organization follows AWS Well-Architected and Security best practices.\n2. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 TECHNICAL ARCHITECTURE DIAGRAM The proposed solution integrates multiple AWS services for CI/CD, automated security analysis, and monitoring. It includes components for source control (GitLab), build and test automation (CodeBuild), pipeline orchestration (CodePipeline), AI-based code review (CodeGuru Reviewer), and centralized alerting (Security Hub + SNS).\nTools used: GitLab\nAWS CodePipeline\nAWS CodeBuild\nAWS CodeGuru Reviewer\nAWS Security Hub, GuardDuty, Detective\nSonarQube, Trivy, Bandit\n2.2 TECHNICAL PLAN The partner will develop buildspec scripts in YAML for CodeBuild to automate:\nSource code scanning (Trivy, Bandit)\nStatic code analysis (CodeGuru Reviewer)\nBuild packaging and deployment triggers\nAll deployments will be version-controlled via GitLab CI triggers. Configuration files will follow Infrastructure as Code principles using AWS CloudFormation.\n2.3 PROJECT PLAN The team will adopt Agile Scrum methodology over 4 sprints (2 weeks each). Stakeholders will participate in Sprint Reviews and Retrospectives.\nRoles and responsibilities:\nDevOps Engineer: CI/CD pipeline setup\nSecurity Engineer: Security integration and analysis\nProject Lead: Coordination, reporting, documentation\nWeekly sync-up meetings will be held via Slack and AWS Chime.\n2.4 SECURITY CONSIDERATIONS Access – IAM least privilege, MFA for admin accounts\nInfrastructure – Private subnets for build agents\nData – S3 encryption (SSE-KMS), CodeBuild log encryption\nDetection – GuardDuty and Security Hub continuous monitoring\nIncident Management – SNS notifications and CloudWatch alarms for anomalies\n3. ACTIVITIES AND DELIVERABLES 3.1 ACTIVITIES AND DELIVERABLES Project Phase Timeline Activities Deliverables/Milestones Total Man-days Assessment Week 1–2 Analyze existing CI/CD Report \u0026amp; Architecture Design 5 Setup base infrastructure Week 3–4 Create CodePipeline \u0026amp; CodeBuild Pipeline Deployed 7 Integrate Security Tools Week 5–6 Add SonarQube, Trivy, Bandit Security Scan Active 6 Monitoring Setup Week 7 Connect Security Hub, CloudWatch Alert System Operational 4 Testing \u0026amp; Go-Live Week 8 Final testing, documentation Go-Live Report 3 Handover Week 9 Knowledge transfer Final Project Handover 2 3.2 OUT OF SCOPE On-premises application security scanning\nThird-party compliance frameworks beyond AWS tools\nNon-AWS CI/CD environments\n3.3 PATH TO PRODUCTION The Proof-of-Concept focuses on AWS-native DevSecOps integration. For production deployment, additional steps such as multi-account security setup, network isolation, and automated patching will be required.\n4. EXPECTED AWS COST BREAKDOWN BY SERVICES Service Description Estimated Monthly Cost (USD) CodePipeline Orchestration 10 CodeBuild Build + Scan 30 CodeGuru Reviewer Code analysis 25 Security Hub Aggregation + Alerts 15 CloudWatch Logs + Metrics 10 S3 Artifact storage 5 SNS Notifications 5 Total (approx.) 100 USD/month 5. TEAM Name Student ID Email / Contact Lê Công Cảnh SE183750 canhlcse183750@fpt.edu.vn Phùng Gia Đức SE183187 ducpgse183187@fpt.edu.vn Vũ Nguyễn Bình SE193185 vunguyenbinh25@gmail.com Lê Minh Dương SE184079 duonglmse184079@fpt.edu.vn Nguyễn Phi Duy SE180529 duynpse180529@fpt.edu.vn 6. RESOURCES \u0026amp; COST ESTIMATES Resource Responsibility Rate (USD/hr) Total Hours Cost (USD) Solution Architect Design \u0026amp; Review 60 40 2400 DevOps Engineer Pipeline Implementation 45 60 2700 Security Engineer Tool Integration 50 50 2500 Total 150 7600 7. ACCEPTANCE Upon completion of each phase, the provider will submit deliverables to the customer with an Acceptance Form. The customer will review within 8 business days and provide either:\nWritten acceptance confirmation, or\nRejection notice with feedback.\nIf no response is received within the acceptance period, the deliverable is deemed accepted.\n"},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/5-workshop/5.2-pipline-flow/","title":"Pipline-flow","tags":[],"description":"","content":"CodePipeline Flow (GitLab → AWS) 1. Source Stage CodePipeline is configured to listen for commits from GitLab (branch: main).\nWhen there is a new commit, the pipeline is automatically activated.\nOutput: source artifact to transfer to the build step.\n2. Build Stage (CodeBuild) Run the build environment according to buildspec.yml.\nExecute:\nInstall Sonar Scanner.\nScan all source code.\nSend results to SonarQube Server.\nIf the build fails → the pipeline stops.\n3. Post-Build Actions CodeBuild pushes build metadata (log, status) to CodePipeline.\nNotify optional: Amazon EventBridge / SNS / Slack.\n"},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/5-workshop/5.3-sonarqube-analysis/","title":"Sonarqube-analysis","tags":[],"description":"","content":"SonarQube Analysis Architecture Sonar Scanner → SonarQube During the build phase, Sonar Scanner performs:\nCode quality analysis. Bug detection. Security vulnerability detection (Security Hotspots, Vulnerabilities). Code smell, complexity, duplication calculation. Results are sent to:\nSonarQube Server (EC2) SonarQube processes and saves the results to its database. Webhook Trigger When the analysis is complete:\nSonarQube makes an HTTP POST call to the API Gateway endpoint. Payload contains: Project name Quality Gate status (Pass/Fail) Error summary (bugs, vulnerabilities, code smells,…) "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 – Solution for Handling DCA Complaints in the Motor Finance Industry Using AWS Services In the context of scientific research increasingly relying on large-scale computational power, many research institutes and universities face overload issues with their High Performance Computing (HPC) infrastructure. Breakthrough projects — especially in fields like genomics, AI/ML, climate modeling, or computational chemistry — are often delayed simply due to a lack of GPU or specialized memory resources.\nThis article introduces how AWS Cloud Bursting helps eliminate computational bottlenecks by automatically and cost-efficiently extending on-premises HPC clusters to the cloud. This enables research teams to access cutting-edge resources such as the latest NVIDIA GPUs, Trainium, Inferentia, Graviton, or even quantum computing through AWS — exactly when needed.\nThrough an analysis of current challenges, architecture, real-world examples, and benefits for both researchers and IT teams, the article provides a comprehensive view of how Cloud Bursting is shaping the future of research computing. It represents an inevitable trend that preserves research momentum and ensures groundbreaking ideas are not constrained by infrastructure limitations.\nBlog 2 – Research at the Speed of Discovery: AWS Cloud Bursting Eliminates Compute Bottlenecks This article explores how AWS Cloud Bursting is transforming the way research institutions and universities handle massive computational workloads in modern science. As breakthroughs in genomics, AI/ML, climate simulation, or computational chemistry increasingly demand powerful and flexible GPU resources, traditional HPC infrastructure often fails to keep up, leading to long wait times and slow research progress.\nAWS Cloud Bursting delivers a powerful solution: extending on-premises HPC clusters to the cloud automatically and cost-effectively, allowing research teams to instantly access next-generation GPUs, specialized AI processors, high-performance file systems, and even quantum computing.\nThe article analyzes current challenges, how HPC bursting works, real-world benefits, and why this strategy is becoming essential for maintaining innovation speed in scientific research.\nBlog 3 – AWS Transform for .NET Now Supports Q\u0026amp;A for Assessment and Transformation Reports This article introduces a new feature in AWS Transform for .NET that makes modernizing .NET applications faster and more intuitive. Instead of manually reading through long and complex assessment and transformation reports, users can now ask questions directly in natural language using the integrated chat interface in the web console.\nThe Q\u0026amp;A feature helps development teams quickly extract critical information about repositories, complexity levels, transformation changes, and upgraded packages — without manually parsing each report.\nThis marks an important step forward in automating and simplifying large-scale .NET modernization, particularly for organizations working with many repositories in major transformation projects.\n"},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in 3 events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS CLOUD MASTER SERIES #1\nDate \u0026amp; Time: 08:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AWS CLOUD MASTER SERIES #2\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS CLOUD MASTER SERIES #3\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/5-workshop/5.4-lambda-and-notification/","title":"Lambda-and-notification","tags":[],"description":"","content":"Lambda Processing + Notification API Gateway → Lambda API Gateway receives webhook from SonarQube and passes payload to Lambda.\nLambda does:\nParse data from SonarQube. Format message (project, error type, severity). Send content to SNS topic. SNS Notifications SNS sends email to dev team:\nReport security errors. Report code errors. Report Quality Gate failures. Link directly to SonarQube dashboard. Benefits:\nDev knows errors immediately. Reduced debugging time. Increase software quality and safety. "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Deploy and operate Amazon Lightsail for websites and databases. Configure Auto Scaling Groups to automatically scale web application resources. Overview of Machine Learning and Generative AI training for partners. Threat detection and incident response using cloud-native services. Use Amazon CloudWatch for monitoring and system analysis. Tasks to be carried out this week: Day Task Date Reference Material 2 - Amazon Lightsail - Hands-on: + Deploy a Lightsail MySQL database for application storage + Create and configure a WordPress instance (Ubuntu, networking, WordPress setup) + Deploy and validate a Prestashop e-commerce instance 06/10/2025 https://000045.awsstudygroup.com/en/ 3 - Auto Scaling Group Hands-on: + Prepare network and infrastructure components + Launch EC2 and RDS for the application + Load data into the DB and deploy the web app + Collect metrics for predictive scaling via CloudWatch + Create Launch Template and configure Load Balancer + Target Group + Verify operation and test manual scaling 07/10/2025 https://000006.awsstudygroup.com/en/ 4 - Introduction to Machine Learning Hands-on: + Overview of AI/ML on AWS + Amazon Rekognition and identity-verification demo + Generative AI overview, use cases and demos - Gen AI training for partners - Hands-on: + Generative AI concepts and application scenarios + Prototyping and solution sharing + GenAI demo for FSI domain 08/10/2025 https://specialforce.awsstudygroup.com/#/course/week-14:-introduction-to-machine-learning 5 - Threat detection \u0026amp; incident response using cloud-native services - Hands-on: + Incident response lifecycle: preparation, detection, collection, containment, analysis, remediation, recovery + Practice response steps and assessment 09/10/2025 https://specialforce.awsstudygroup.com/#/course/week-16:-threat-detection-and-incident-response-using-cloud-native-services 6 - Amazon CloudWatch - Hands-on: + Query and analyze Metrics + Perform metric math and create Dynamic Labels + Use Logs Insight to read logs + Filter metrics, configure Alarms and build Dashboards for monitoring 10/10/2025 https://000008.awsstudygroup.com/en/ Week 5 Achievements: Amazon Lightsail Successfully deployed a Lightsail MySQL database to store application data. Installed and configured a WordPress instance on Ubuntu, including networking and WordPress setup. Deployed a Prestashop e-commerce instance and validated its stable operation. Auto Scaling Group Completed network configuration, launched EC2 and RDS, loaded data and deployed the web application. Created a Launch Template, configured a Load Balancer and Target Group, and deployed an Auto Scaling Group to manage load. Prepared metrics for predictive scaling via CloudWatch and tested manual scaling; configured email notifications where appropriate. Machine Learning \u0026amp; Generative AI Gained foundational knowledge of AI/ML on AWS and experimented with Amazon Rekognition for recognition/verification use-cases. Explored Generative AI, RAG patterns and agent models; completed assessment quizzes. Threat Detection \u0026amp; Incident Response Understood and practiced the incident response lifecycle: preparation, detection, collection, containment, analysis, remediation and recovery. Learned to use AWS native security services to detect and respond to threats. Amazon CloudWatch Proficient in querying, filtering and analyzing Metrics/Logs; configured Alarms and built Dashboards for proactive monitoring. "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/5-workshop/5.5-closed-loop-devsecops/","title":"Closed-loop-devsecops","tags":[],"description":"","content":"DevSecOps Feedback Loop (Closed Cycle) The system creates a closed DevSecOps loop: Dev commits code → GitLab ↓ CodePipeline runs → CodeBuild scans → SonarQube analyzes ↓ Webhook → Lambda → SNS sends email ↓ Dev receives error → Fix → Commit again ↓ Return to loop\nMain benefits Automatically detects bugs and security vulnerabilities. Real-time alerts help devs fix immediately. Ensures code quality throughout the development lifecycle. DevSecOps culture in practice: “Security shift-left”. Conclusion Pipeline ensures a modern, automated, secure and sustainable development process. Each commit is rigorously analyzed, helping the dev team maintain high quality standards.\n"},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"DevSecOps Security Scan Pipeline on AWS Overview In this workshop, we build a DevSecOps pipeline on AWS to integrate security right into the CI/CD process instead of manually handling it at the end of the project. Each time a developer pushes code to the repository, the system will automatically trigger security and source code quality checks.\nMain objectives Automatically scan for security vulnerabilities after each commit/push of code. Perform container image and dependency scanning using Trivy, helping to detect vulnerabilities in libraries and base images. Run Bandit to check for security issues in Python/JavaScript code. Integrate SonarQube to evaluate code quality (code smells, duplication, coverage, maintainability\u0026hellip;). Push important findings to AWS Security Hub to centrally manage security alerts. Send real-time notifications (via Email/SNS/Chat\u0026hellip;) when detecting serious problems for the development team to handle promptly.\nContent Workshop overview Pipline flow Sonarqube analysis Lambda and notification Close loop devsecops "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Build highly available web applications (High Availability). Practice Amazon Route53 for DNS management. Use AWS CLI to manage resources on EC2. Develop and deploy AWS Lambda functions. Migrate and transfer virtual machines from on-premises to AWS. Tasks to be carried out this week: Day Task Date Reference Material 2 - Amazon Route53 - Hands-on: + Create KeyPair for secure connections + Build CloudFormation Template for automation + Configure Security Group to control traffic + Connect via RDGW using RDP and log in to server + Deploy Microsoft Active Directory (AD) + Set up Outbound Rules, Resolver Rules, Inbound Endpoints for DNS Resolution 13/10/2025 https://000010.awsstudygroup.com/en/ 3 - AWS CLI on EC2 - Hands-on: + Install AWS CLI on Linux + List and manage EC2 resources via CLI + Use CLI to interact with S3, SNS + Create and manage IAM identities + Initialize VPC, Internet Gateway, EC2 Instance entirely via command line 14/10/2025 https://000011.awsstudygroup.com/en/ 4 - AWS VM Import/Export - Hands-on: + Set up VMware Workstation to manage virtual machines + Successfully migrate VM from on-premises to AWS + Create EC2 Instance from imported AMI + Configure S3 Bucket ACL for access management + Export VM from EC2 Instance/AMI back to local machine 15/10/2025 https://000014.awsstudygroup.com/en/ 5 - AWS Lambda - Hands-on: + Prepare VPC, Security Group, EC2 Instance + Integrate Slack Webhook for enhanced monitoring + Tag instances + Create IAM Role for Lambda + Develop Lambda functions (Start/Stop) to automatically control EC2 + Verify operation and clean up resources 16/10/2025 https://000022.awsstudygroup.com/en/ 6 - Highly Available Web Application Workshop - Hands-on: + Prepare Security Group for EC2 and Database + Launch EC2 and RDS Database + Install WordPress on EC2 + Build Auto Scaling including: AMI, Launch Template, Target Group, Load Balancer, Auto Scaling Group + Snapshot Database and perform Recovery + Integrate CloudFront for acceleration 17/10/2025 https://000021.awsstudygroup.com/en/ Week 6 Achievements: Amazon Route53 Successfully created KeyPair, CloudFormation Template, and Security Group for network architecture. Set up remote connections via RDGW/RDP and accessed servers. Deployed Microsoft Active Directory and configured DNS components (Outbound/Inbound Endpoints, Resolver Rules) in Route53. AWS CLI on EC2 Installed AWS CLI on Linux and practiced commands for managing EC2, S3, SNS, IAM, VPC. Mastered automation to create Internet Gateway and EC2 Instance via CLI. Highly Available Web Application Workshop Built WordPress web application with High Availability architecture including Auto Scaling, Load Balancer, CloudFront. Configured AMI, Launch Template, Target Group to support automatic scaling based on demand. Practiced Database Snapshot and Recovery; managed resources efficiently. AWS VM Import/Export Successfully migrated VMware Workstation virtual machine to AWS EC2 Instance. Configured S3 Bucket ACL and practiced exporting VM back to local machine; understood hybrid migration process. AWS Lambda Created Lambda functions (Start/Stop) to automatically manage EC2 instance state; integrated Slack Webhook for monitoring. Verified successful operation and cleaned up resources after completing the lab. "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at FCJ (First Cloud Journey) AWS from 08/09/2025 to 09/12/2025, I had the opportunity to learn, practice, and apply the knowledge gained from school in a real working environment.\nI participated in events and worked on projects using AWS services, which helped me improve my analytical skills, communication, technical knowledge, and reporting abilities.\nIn terms of work attitude, I always tried to complete tasks well, follow company rules, and actively communicate with colleagues to enhance work efficiency.\nTo objectively reflect my internship performance, I would like to provide a self-evaluation based on the criteria below:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Areas for Improvement Improve discipline and strictly follow company or organizational regulations Enhance problem-solving thinking and approach Develop better communication skills in daily interactions and in work-related situations "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Organize AWS resources using Tags and Resource Groups. Apply AWS Systems Manager for automation and operations. Manage resource access through IAM policies. Practice system monitoring with Amazon CloudWatch and Grafana. Use Session Manager to securely access servers. Tasks to be carried out this week: Day Task Date Reference Material 2 - Amazon CloudWatch and Grafana - Hands-on: + Initialize VPC, Subnets, Security Groups, EC2, IAM User and Role + Install Grafana on EC2 via SSH connection + Integrate CloudWatch into Grafana for centralized monitoring 20/10/2025 https://000029.awsstudygroup.com/en/ 3 - AWS Systems Manager – Patch Manager - Hands-on: + Prepare VPC, Subnet, Windows EC2, IAM Role + Configure Patch Manager for patch management + Use Run Command to automatically execute commands on EC2 21/10/2025 https://000031.awsstudygroup.com/en/ 4 - Tags and Resource Groups - Hands-on: + Create EC2 Instance and apply Tags + Use Resource Groups to filter resources by Tag + Set up IAM Policy to control access based on Tags + Test Switch Role and verify policies 22/10/2025 https://000027.awsstudygroup.com/vi/ https://000028.awsstudygroup.com/en/ 5 - AWS Systems Manager – Session Manager - Hands-on: + Create VPC, Subnet, Security Group, Linux EC2, IAM Role + Enable DNS hostname and create VPC Endpoints (ssm, ssmmessages, ec2messages) + Initialize Private EC2 and configure IAM Role + Create S3 Gateway Endpoint and S3 Bucket for session logs + Use Session Manager to access EC2 and perform Port Forwarding 23/10/2025 - 24/10/2025 https://000058.awsstudygroup.com/en/ Week 7 Achievements: Amazon CloudWatch and Grafana Set up centralized monitoring system using Grafana, integrating data from CloudWatch to analyze system operations. Tags and Resource Groups Master resource organization skills using Tags, leverage Resource Groups for efficient management and security optimization. AWS Systems Manager – Patch Manager Proficient in patch management automation and Windows system operations, ensuring security and stability. AWS Systems Manager – Session Manager Access and manage Private Network servers without SSH/RDP, enhance security and centralized administration on AWS. "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What were you most satisfied with during your internship? Having the opportunity to experience a large enterprise environment and learn a lot about cloud technologies. What do you think the company should improve for future interns? Providing drinking water for interns. If recommending to your friends, would you encourage them to intern here? Why? Absolutely yes, because FCJ is a great starting point for those who have a passion for cloud. Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Sharing more in-depth knowledge about cloud security (CloudSec). Would you like to continue this program in the future? Yes, I would like to continue if possible. "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Practice configuring Amazon SSO (IAM Identity Center) for a multi-account environment. Become familiar with and deploy resources using Amazon CloudFormation. Apply IAM Permission Boundaries to limit identity permissions. Practice creating IAM Roles with Conditions to refine access restrictions. Enable and use AWS Security Hub for security posture assessments. Configure AWS WAF to protect web applications from common attacks. Tasks to be carried out this week: Day Task Date Reference Material 2 - IAM Role \u0026amp; Condition - Hands-on: + Create sample IAM Roles and IAM Users + Inspect and analyze permissions of created identities + Create an Admin Role and try Switch Role + Apply Conditions to restrict access by IP and by time window 27/10/2025 https://000044.awsstudygroup.com/en/ https://000018.awsstudygroup.com/en/ 3 - Amazon SSO (Identity Center) - Hands-on: + Use AWS Organizations to create/manage AWS Accounts + Configure IAM Identity Center and create Users/Groups + Create Permission Sets, assign to accounts and verify access with time-based controls + Practice customer managed policies and Identity Store APIs 28/10/2025 https://000030.awsstudygroup.com/en/ 4 - Amazon CloudFormation - Hands-on: + Prepare IAM Roles and Users for deployment + Create a workspace and author templates on Cloud9 + Deploy a Lambda function, create a Stack and connect EC2 resources + Use StackSets for multi-region deployments and run Drift Detection 29/10/2025 https://000037.awsstudygroup.com/en/ 5 - IAM Permission Boundary \u0026amp; AWS WAF - Hands-on: + Create a Permission Boundary policy and apply it to an IAM User + Observe the behavior of a permission-limited user + Create an S3 bucket and host a sample web page + Configure Web ACLs with managed rules and a custom rule; test the rule and log requests via WAF 30/10/2025 Week 8 Achievements: Prepared tooling and documentation environment Installed and configured supporting tools: Hugo theme, Snagit, ActivePresenter, and Draw.io for note-taking, recording, and architecture diagrams. Set up a ready practice environment for writing guides and reports. IAM Role \u0026amp; Conditions – AWS Security Hub Gained experience creating, managing, and constraining IAM permissions; used AWS Security Hub to assess compliance against AWS Foundational Security Best Practices. Amazon SSO – IAM Identity Center Deployed an SSO model across multiple AWS accounts, managed users and permission sets, including time-based access controls. Amazon CloudFormation Practiced automating infrastructure deployment with CloudFormation, managed multi-region deployments with StackSets, and tracked drift. IAM Permission Boundary \u0026amp; AWS WAF Applied Permission Boundaries to limit identity scopes; configured AWS WAF (Web ACLs, managed rules, and custom rules) to protect web applications and logged requests for analysis. "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Master AWS KMS and encryption key lifecycle management. Deploy and operate automated backup workflows using AWS Backup. Research, configure, and establish connectivity between VPCs using AWS Transit Gateway. Get familiar with Docker and deploy containerized applications on Amazon ECS. Tasks to be completed this week: Day Task Date Reference Material 3 - AWS KMS – Hands-on: + Create policy and role, configure sample groups and users + Create a CMK (Customer Master Key) and test encrypt/decrypt operations on S3 objects + Enable CloudTrail logging and query logs using Athena 11/03/2025 https://000033.awsstudygroup.com/en/ https://000013.awsstudygroup.com/en/ 4 - AWS Backup – Hands-on: + Prepare S3/resources templates and CloudFormation stack for backup + Configure Backup Plan, Backup Vault, and SNS notifications + Test restore to validate data availability 4/11/2025 https://000013.awsstudygroup.com/en/ https://000019.awsstudygroup.com/en/ 5 - Transit Gateway – Hands-on: + Create KeyPair and prepare CloudFormation template + Deploy Transit Gateway, create attachments and route tables + Add routes to VPC route tables and verify network connectivity 5/11/2025 https://000020.awsstudygroup.com/en/ https://000015.awsstudygroup.com/en/ 6 - Docker \u0026amp; Amazon ECS – Application Deployment: + Build Docker image for demo application + Push image to ECR + Create task definition and ECS service to run the container 6/11/2025 https://000016.awsstudygroup.com/en/ https://000015.awsstudygroup.com/en/ Week 9 Achievements: Successfully configured and tested AWS WAF; collected request logs for analysis. Created and managed CMK, applied S3 object encryption, monitored activity using CloudTrail, and analyzed logs with Athena. Set up a complete AWS Backup environment including Backup Plan, Backup Vault, SNS notifications, and validated successful restore operations. Deployed Transit Gateway, enabling communication between multiple VPCs with proper routing verification. Gained understanding of Docker packaging and deployed containerized applications on ECS; familiarized with ECS task/service architecture. "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Build a CI/CD pipeline using AWS CodePipeline/CodeBuild. Optimize configuration and cost efficiency for Amazon EC2. Visualize and analyze AWS cost usage. Deploy containerized applications on AWS. Design and test a Data Lake on AWS. Tasks to be completed this week: Day Task Date Reference Material 2 - EC2 Optimization – Hands-on: + Monitor EC2 metrics in CloudWatch + Create IAM Role for CloudWatch Agent + Collect optimization data and apply recommendations from Compute Optimizer + Resize/reconfigure EC2 instance type accordingly 10/11/2025 https://000032.awsstudygroup.com/en/ https://000040.awsstudygroup.com/en/ 3 - AWS CodePipeline \u0026amp; CI/CD – Hands-on: + Create repository and push sample source code + Configure CodeBuild and integrate GitHub (Actions/Access Key) + Set up CodePipeline to deploy to ECS + Route logs using FireLens, store logs in S3, and monitor via CloudWatch 11/11/2025 https://000017.awsstudygroup.com/en/ 4 - Deploy Docker Application on AWS – Hands-on: + Install dependencies to run the application locally + Prepare VPC, Security Group, IAM Roles; authenticate Docker Hub/ECR + Prepare RDS and DB Subnet Group + Configure Ubuntu EC2 and deploy backend/frontend using Docker/Docker Compose 12/11/2025 https://000024.awsstudygroup.com/en/ https://000015.awsstudygroup.com/en/ 5 - AWS Cost Visualization – Hands-on: + Analyze cost by service and account + Check Saving Plans coverage + Generate reports and charts via Cost Explorer + Export EC2 usage reports and analyze Data Transfer Out 13/11/2025 https://000034.awsstudygroup.com/en/ https://000032.awsstudygroup.com/en/ 6 - Build Data Lake on AWS – Hands-on: + Create IAM Role \u0026amp; policy for data ingestion + Create S3 Bucket for the data lake + Use Kinesis Firehose for ingestion + Create Glue Crawler/Catalog and inspect data on S3 + Analyze using Athena and visualize via QuickSight 14/11/2025 https://000035.awsstudygroup.com/en/ Week 10 Achievements: EC2 optimization: deployed CloudWatch Agent, collected performance metrics, applied Compute Optimizer recommendations, and resized instances effectively. Completed Data Lake design: ingested data via Kinesis Firehose, stored in S3, cataloged with Glue, analyzed using Athena, and created visualizations using QuickSight. Successfully deployed Docker-based applications on AWS: prepared infrastructure (VPC, SG, IAM), configured EC2, ran applications using Docker/Docker Compose, and pushed images to Docker Hub/ECR. Completed AWS cost analysis and visualization: service-level and account-level cost breakdown, Cost Explorer charts, Saving Plans evaluation, and Data Transfer Out analysis. Built CI/CD pipeline: configured CodeBuild/CodePipeline, integrated GitHub, routed logs with FireLens, stored logs to S3, and monitored with CloudWatch. "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Understand Serverless architecture and its implementation on AWS. Master the process of building APIs and Frontend applications using a serverless model. Become familiar with AWS SAM for Infrastructure as Code (IaC) deployments. Develop, deploy, and automate Microservices on AWS. Improve data organization and workflow optimization within Microservices. Explore Messaging \u0026amp; Eventing mechanisms in distributed Microservice architectures. Tasks to be completed this week: Day Task Date References 2 - Serverless – Lambda interacting with S3 \u0026amp; DynamoDB – Hands-on: + Create a Lambda function to process images uploaded to S3 + Create an S3 Bucket for storing images + Write an IAM Policy for the Lambda function + Validate the Lambda Function execution flow + Create a DynamoDB table for data management + Build a Lambda function to write data to DynamoDB - Serverless Frontend \u0026amp; API Gateway – Hands-on: + Deploy a web frontend to an S3 Bucket + Create a DynamoDB table to store information + Build 3 Lambda functions for read/write/delete operations + Configure API Gateway methods \u0026amp; CORS + Test APIs using Postman and from the frontend 17/11/2025 https://000078.awsstudygroup.com/en/ https://000079.awsstudygroup.com/en/ 3 - Build a Microservice – Hands-on: + Create a CloudFormation Stack as the practice environment + Configure EC2 \u0026amp; Eclipse IDE + Initialize a Lambda project using AWS Toolkit + Upload Lambda function to AWS from Eclipse + Build an image-processing Lambda function \u0026amp; deploy via Console + Use Maven \u0026amp; PowerShell for automated microservice deployment + Create a source repository via AWS CodeStar + Integrate CI/CD pipeline and update API Target Region 18/11/2025 https://000052.awsstudygroup.com/en/ 4 - Data restructuring \u0026amp; workflow optimization – Hands-on: + Create a new Key Pair \u0026amp; CloudFormation Stack + Launch and connect to a Windows Instance for development + Add a Global Secondary Index to the DynamoDB table + Create a new CodeStar repository \u0026amp; import project from Eclipse + Rebuild the Microservice project and update API Target Region + Add IAM Policy for the role + Deploy via AWS Pipeline + Upgrade TripSearch Microservice \u0026amp; add Step Functions workflow + Integrate Lambda into workflow \u0026amp; extend calculation logic + Download CalculatorStepFull.zip and import into Eclipse IDE 19/11/2025 https://000053.awsstudygroup.com/en/ 5 AWS SAM – writing templates, packaging, and deploying serverless applications with IaC 20/11/2025 https://000080.awsstudygroup.com/en/ 6 Messaging \u0026amp; Eventing in Microservices – operating models, AWS services, and how to apply them in distributed architectures 21/11/2025 https://000054.awsstudygroup.com/en/ Week 11 Achievements: Successfully built a Serverless system integrating Lambda – S3 – DynamoDB: image processing, IAM policy configuration, DynamoDB read/write operations, and direct validation via AWS Console. Developed a Serverless application combining API Gateway and Frontend: created 3 CRUD Lambda functions, fully configured CORS, tested APIs via Postman, and deployed the frontend to S3 for end-to-end validation. Gained solid understanding of AWS SAM operations: creating templates, packaging, and deploying stacks using Infrastructure as Code. Completed a full Microservice implementation: provisioning environment using CloudFormation, developing with AWS Toolkit in Eclipse, deploying through Maven/PowerShell, managing code with AWS CodeStar, integrating CI/CD, and configuring API Target Region. Reorganized Microservice workflows \u0026amp; data: added GSI for DynamoDB, optimized project source, redeployed via pipeline, expanded TripSearch Microservice, created Step Functions workflows, integrated Lambda functions, and enhanced processing logic. Explored concepts of Microservice Messaging \u0026amp; Eventing and learned how to apply event-driven patterns across AWS distributed systems. "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Project development Tasks to be completed this week: Day Task Date References 2 WORKSHOP 24/11/2025 3 WORKSHOP 25/11/2025 4 WORKSHOP 26/11/2025 5 WORKSHOP 27/11/2025 6 WORKSHOP 28/11/2025 "},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://dd220.github.io/AWS_FCJ_PhungGiaDuc_Report/tags/","title":"Tags","tags":[],"description":"","content":""}]